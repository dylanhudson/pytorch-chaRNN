{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6754176",
   "metadata": {},
   "source": [
    "Start with the imports  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d65ce768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import string\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bafbb9bc",
   "metadata": {},
   "source": [
    "Then we define our TextRNN class, which inherits from the Torch class 'nn.Module'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfc9037",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, hidden_size=128, num_layers=2):\n",
    "        super(TextRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
    "\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, num_layers, batch_first=True)\n",
    "\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        embedded = self.embedding(x)\n",
    "\n",
    "        if hidden is None:\n",
    "            output, hidden = self.lstm(embedded)\n",
    "        else:\n",
    "            output, hidden = self.lstm(embedded, hidden)\n",
    "\n",
    "        output = output.reshape(-1, self.hidden_size)\n",
    "        output = self.fc(output)        \n",
    "\n",
    "        return output, hidden \n",
    "    \n",
    "    def init_hidden(self, batch_size, device):\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, sel.fhidden_size).to(device)\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_size).to(device)\n",
    "        return (h0, c0)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45bbe38",
   "metadata": {},
   "source": [
    "Next, we define a class \"TextDataset\" to load in the data, and handle randomly sampling the input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eded766",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset:\n",
    "    \n",
    "    #seq_length is a hyperparameter that controls the memory context length, and\n",
    "    #thus also affects efficiency. Longer sequences can result in more context and longer\n",
    "    #patterns being learned, but requires more memory and compute time. \n",
    "    def __init__(self, text_file, seq_length=100):\n",
    "        with open(text_file, 'r', encoding='utf-8') as f:\n",
    "            self.text = f.read()\n",
    "\n",
    "        self.chars = sorted(list(set(self.text)))\n",
    "        self.vocab_size = len(self.chars)\n",
    "        self.char_to_idx = {ch: i for i, ch in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: ch for i, ch in enumerate(self.chars)}\n",
    "\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "        self.data = [self.char_to_idx[ch] for ch in self.text]\n",
    "\n",
    "    # Here we create randomly sampled batches of sequences, and put them\n",
    "    # into tensors. In the \"target\" tensor, the sequence is shifted by one character\n",
    "    # from the corresponding input tensor sequence, so we can verify the prediction \n",
    "    # for the next character in the sequence.\n",
    "    #     \n",
    "    def get_batch(self, batch_size):\n",
    "        start_indices = [random.randint(0, len(self.data) - self.seq_length - 1) \n",
    "                         for _ in range(batch_size)]\n",
    "        \n",
    "        inputs = []\n",
    "        targets = []\n",
    "\n",
    "        for start_idx in start_indices:\n",
    "            input_seq = self.data[start_idx:(start_idx + self.seq_length)]\n",
    "            target_seq = self.data[(start_idx + 1): (start_idx + self.seq_length + 1)]\n",
    "            inputs.append(input_seq)\n",
    "            targets.append(target_seq)\n",
    "\n",
    "        return torch.tensor(inputs), torch.tensor(targets)\n",
    "\n",
    "\n",
    "                                                        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74745356",
   "metadata": {},
   "source": [
    "Now we'll add the training function, and the generation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d55262d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(text_file, epochs=100, batch_size=32, learning_rate=0.001, \n",
    "                hidden_size=128, num_layers=2, seq_length=100):\n",
    "    \n",
    "    #device initialization - could add support if on Apple Silicon\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f'Device initalialized: {device}')\n",
    "\n",
    "    dataset = TextDataset(text_file, seq_length)\n",
    "    print(f'Vocab size: {dataset.vocab_size}')\n",
    "    print(f'Text length: {len(dataset.text)}')\n",
    "\n",
    "    model = TextRNN(dataset.vocab_size, hidden_size, num_layers).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    print('Starting training...')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "\n",
    "        num_batches = 20\n",
    "\n",
    "        for batch in range(num_batches):\n",
    "            inputs, targets = dataset.get_batch(batch_size)\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            # clearing the gradients for each batch - if they accumulated, we'd end up\n",
    "            # with huge update gradients, instability, and massive loss. \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # this function runs the model on the inputs, returning the predictions\n",
    "            outputs, _ = model(inputs)\n",
    "\n",
    "            targets = targets.reshape(-1)\n",
    "\n",
    "            # calculating the loss (uses nn.CrossEntropyLoss)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            loss.backward()  # backpropagation to calculate gradients\n",
    "            optimizer.step() # update the model parameters\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / num_batches\n",
    "\n",
    "        #print the avg loss every 10 epochs\n",
    "        if (epoch +1) % 10 == 0:\n",
    "            print(f'Epoch [{epoch + 1}/{epochs}], Loss: {avg_loss:.4f}')\n",
    "    \n",
    "    print('Training complete.')\n",
    "\n",
    "    return model, dataset\n",
    "\n",
    "def generate_text(model, dataset, device, seed_text='', length=100, temperature=1.0):\n",
    "    model.eval()\n",
    "\n",
    "    if not seed_text:\n",
    "        seed_text = random.choice(dataset.text)\n",
    "    \n",
    "    current_seq = [dataset.char_to_idx.get(ch, 0) for ch in seed_text]\n",
    "    generated = seed_text\n",
    "\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "\n",
    "        for _ in range(length):\n",
    "\n",
    "            if len(current_seq) > dataset.seq_length:\n",
    "                current_seq = current_seq[-dataset.seq_length]\n",
    "                \n",
    "            input_tensor = torch.tensor([current_seq]).to(device)\n",
    "            \n",
    "            output, hidden = model(input_tensor, hidden)\n",
    "\n",
    "            last_output = output[-1] / temperature\n",
    "            probabilities = torch.softmax(last_output, dim=0)\n",
    "\n",
    "            next_char_idx = torch.multinomial(probabilities, 1).item()\n",
    "            next_char = dataset.idx_to_char[next_char_idx]\n",
    "\n",
    "            generated += next_char\n",
    "\n",
    "    return generated\n",
    "\n",
    "        \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf89993",
   "metadata": {},
   "source": [
    "Generate a sample file if we need it for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "218d7fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sample_training_text = \"\"\" Far out in the uncharted backwaters of the unfashionable end of the western spiral arm of the Galaxy lies a small unregarded yellow sun. Orbiting this at a distance of roughly ninety-two million miles \n",
    "                        is an utterly insignificant little blue-green planet whose ape-descended life forms are so amazingly primitive that they still think digital watches are a pretty neat idea.\n",
    "                        This planet has—or rather had—a problem, which was this: most of the people living on it were unhappy for pretty much all of the time. Many solutions were suggested for this problem, \n",
    "                        but most of these were largely concerned with the movement of small green pieces of paper, which was odd because on the whole it wasn't the small green pieces of paper that were unhappy. Many were increasingly of the opinion that they'd all made a big mistake in coming down from the trees in the first place. And some said that even the trees had been a bad move, and that no one should ever have left the oceans.\n",
    "In many of the more relaxed civilizations on the Outer Eastern Rim of the Galaxy, the Hitchhiker's Guide has already supplanted the great Encyclopaedia Galactica as the standard repository of all knowledge and wisdom, for though it has many omissions and contains much that is apocryphal, or at least wildly inaccurate, it scores over the older, more pedestrian work in two important respect\"\"\" * 20\n",
    "\n",
    "with open('sample_text.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_training_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1132a988",
   "metadata": {},
   "source": [
    "Train the model\n",
    "(The sample file took about 2 minutes on an Intel i9 CPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f5109df1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device initalialized: cpu\n",
      "Vocab size: 42\n",
      "Text length: 28760\n",
      "Starting training...\n",
      "Epoch [10/50], Loss: 1.1859\n",
      "Epoch [20/50], Loss: 0.2273\n",
      "Epoch [30/50], Loss: 0.1219\n",
      "Epoch [40/50], Loss: 0.0928\n",
      "Epoch [50/50], Loss: 0.0763\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "model, dataset = train_model(\"sample_text.txt\", epochs=50, batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e985cd3",
   "metadata": {},
   "source": [
    "Generate some text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "477aad61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: The meaning of life isttttttttttttttttttttttttstttttttttpttttttpttttttttpttttttttptttttttttutptttttttttttttttttttttttttttt\n"
     ]
    }
   ],
   "source": [
    "generated_text = generate_text(model, dataset, torch.device('cuda' if torch.cuda.is_available() else 'cpu'),\n",
    "                               seed_text='The meaning of life is', length=100, temperature=1)\n",
    "print(f\"Generated Text: {generated_text}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
